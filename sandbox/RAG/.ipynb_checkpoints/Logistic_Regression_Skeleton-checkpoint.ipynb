{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de3cc4-3c7c-479f-84df-4ec95e4d0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Logistic Regression (from scratch) â€” Skeleton\n",
    "# ================================\n",
    "\n",
    "# 0) Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1) Load / prepare data\n",
    "# --------------------------------------------------\n",
    "# Assumptions (edit as needed):\n",
    "#   - Binary classification with labels in {0,1}\n",
    "#   - X is an (m, n) feature matrix (no bias column yet)\n",
    "#   - y is an (m,) label vector\n",
    "\n",
    "# TODO: replace with your real data loading\n",
    "def load_data():\n",
    "    # EXAMPLE PLACEHOLDER â€” REPLACE with file read:\n",
    " \n",
    "\n",
    "    # (Optional) Feature scaling â€” often helps training stability\n",
    "\n",
    "    # (Optional) Split the dataset into training and test\n",
    "\n",
    "    \n",
    "# Add bias column (x0 = 1)\n",
    "X = np.column_stack([np.ones(m), X])  # shape: (m, n+1)\n",
    "n_with_bias = X.shape[1]\n",
    "\n",
    "\n",
    "# 2) Utility functions: sigmoid, loss, gradient, prediction\n",
    "# --------------------------------------------------\n",
    "def sigmoid(z):\n",
    "    # Ïƒ(z) = 1 / (1 + e^-z)\n",
    "    # TODO: Complete the sigmoid function\n",
    "\n",
    "def predict_proba(X, w):\n",
    "    # p = Ïƒ(Xw)\n",
    "    # TODO: Complete the predict probability function\n",
    "\n",
    "def binary_cross_entropy(y_true, y_prob, eps=1e-12):\n",
    "    # Log-loss = - [ y log(p) + (1-y) log(1-p) ]\n",
    "    # You might need to restrict the values using np.clip\n",
    "    # TODO: Complete the binary cross entropy function\n",
    "    \n",
    "def gradient(X, y_true, y_prob):\n",
    "    # âˆ‚J/âˆ‚w = (1/m) X^T (p - y)\n",
    "    # TODO: Complete the gradient descent function\n",
    "\n",
    "\n",
    "# 3) Initialize parameters\n",
    "# --------------------------------------------------\n",
    "# TODO: set the initial values for the parameters\n",
    "    \n",
    "# 4) Hyperparameters\n",
    "# --------------------------------------------------\n",
    "# TODO: set learning_rate and num_iterations\n",
    "\n",
    "# For tracking\n",
    "cost_history = []\n",
    "\n",
    "\n",
    "# 5) Gradient Descent loop\n",
    "# --------------------------------------------------\n",
    "\n",
    "    # TODO: Forward pass: compute probabilities\n",
    "    \n",
    "    # TODO: Compute loss (log-loss / cross-entropy)\n",
    "    \n",
    "    # TODO: Backward pass: compute gradient\n",
    "    \n",
    "    # TODO Parameter update\n",
    "    \n",
    "\n",
    "    \n",
    "# 6) Final parameters\n",
    "# --------------------------------------------------\n",
    "print(\"Final parameters (w):\")\n",
    "print(w)  # w[0] is bias term\n",
    "\n",
    "\n",
    "# 7) Plot: Cost vs Iterations\n",
    "# --------------------------------------------------\n",
    "plt.figure()\n",
    "plt.plot(range(len(cost_history)), cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost (Log-Loss)\")\n",
    "plt.title(\"Training: Cost vs. Iterations\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 8) Plot: Cost vs 3 of the most important parameters\n",
    "# --------------------------------------------------\n",
    "# Idea: pick 3 parameters (excluding bias) by magnitude after training,\n",
    "# then sweep each parameter around its learned value while holding others fixed,\n",
    "# and plot the (1D) cost curve for each.\n",
    "#At the end of training, you have a weight vector w (parameters). You also have a \n",
    "# cost function ð½ ( ð‘¤)\n",
    "#J(w), which tells you how good or bad those weights are.\n",
    "\n",
    "#Normally, we look at cost vs iterations (how the loss decreases over time). But sometimes itâ€™s useful to see:\n",
    "# â€œWhat happens to the cost if I wiggle just one parameter, while keeping all others fixed?â€\n",
    "\n",
    "# Thatâ€™s what the sweep does.\n",
    "#For each candidate value in the sweep, we create a temporary weight vector w_tmp.\n",
    "# We replace the chosen parameter with val.\n",
    "# Then we compute the cost ð½ (ð‘¤_tmp ) using the same dataset.\n",
    "# Append that cost to a list.\n",
    "\n",
    "#We plot:\n",
    "\n",
    "#x-axis: the parameterâ€™s value (from the sweep)\n",
    "\n",
    "#y-axis: the cost computed at that parameter value\n",
    "\n",
    "#This shows the â€œsensitivity curveâ€ of the cost with respect to that parameter.\n",
    "\n",
    "# Identify top-3 parameters by |w| (excluding bias at index 0)\n",
    "\n",
    "param_indices = np.argsort(np.abs(w[1:]))[::-1][:3] + 1  # shift by 1 to skip bias\n",
    "print(\"Plotting cost sensitivity for parameter indices:\", param_indices)\n",
    "\n",
    "def compute_cost_given_w(mod_w):\n",
    "    # Helper to compute cost for a modified parameter vector\n",
    "    y_hat_mod = predict_proba(X, mod_w)\n",
    "    return binary_cross_entropy(y, y_hat_mod)\n",
    "\n",
    "# For each chosen parameter, sweep values and compute cost\n",
    "for idx in param_indices:\n",
    "    center = w[idx]\n",
    "    #  choose a sensible sweep range; here: Â±1.0 around the trained value\n",
    "    sweep = np.linspace(center - 1.0, center + 1.0, 60)\n",
    "\n",
    "    costs = []\n",
    "    for val in sweep:\n",
    "        w_tmp = w.copy()\n",
    "        w_tmp[idx] = val\n",
    "        costs.append(compute_cost_given_w(w_tmp))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(sweep, costs)\n",
    "    plt.xlabel(f\"Parameter w[{idx}]\")\n",
    "    plt.ylabel(\"Cost (Log-Loss)\")\n",
    "    plt.title(f\"Cost vs Parameter w[{idx}] (holding others fixed)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 9) (Optional) Inference helper\n",
    "# --------------------------------------------------\n",
    "def predict_label(X_new, w, threshold=0.5):\n",
    "    # Returns 0/1 predictions based on threshold\n",
    "    return (predict_proba(X_new, w) >= threshold).astype(int)\n",
    "\n",
    "# Example usage (remove in production):\n",
    "# preds = predict_label(X, w)\n",
    "# accuracy = (preds == y).mean()\n",
    "# print(\"Training accuracy (for reference only):\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
